{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Movie Information\n",
    "Extract movie names and IMDB ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import omdb\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set path for PhantomJS lightweight browser, as well as window size to be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.PhantomJS(executable_path='C:/phantomjs-2.1.1-windows/bin/phantomjs.exe')\n",
    "driver.set_window_size(1024,768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a function to set the URL of the page to be scraped **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_url(URL):\n",
    "    driver.get(URL)\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** This will be the main scraping function**\n",
    "1. Find the column in the imdb website with the div called 'list compact'\n",
    "2. Look for the two pieces of information needed (names & id)\n",
    "3. Append the two generated lists into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_panel(soup):\n",
    "    col = soup.find('div', class_='list compact')\n",
    "    \n",
    "    names = []\n",
    "    imdbID = []\n",
    "\n",
    "    #------------------------------------------------------------#\n",
    "    for n in col.find_all(\"td\",class_=\"title\"):\n",
    "        try:\n",
    "            names.append(n.text.encode('ascii','ignore'))\n",
    "        except:\n",
    "            names.append(None)\n",
    "    #------------------------------------------------------------#\n",
    "    for i in col.find_all(\"td\",class_=\"title\"):\n",
    "        try:\n",
    "            imdbID.append(i.find_all('a')[0])\n",
    "        except:\n",
    "            imdbID.append(None)\n",
    "    #------------------------------------------------------------#\n",
    "    data = pd.DataFrame({'name': names, 'id': imdbID})\n",
    "    #------------------------------------------------------------#\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate links to scrape through**\n",
    "\n",
    "Found that the website lists the movies in increments of 250, so a generated list of links was made. It will generate the page extraction for up to 10,000 movies which is about how many movies are in the linked page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for link in range (1,10000,250):\n",
    "    links.append('http://www.imdb.com/list/ls057823854/?start='+str(link)+'&view=compact&sort=listorian:asc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scrape through all the generated links and save the file **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished page 1\n",
      "Finished page 2\n",
      "Finished page 3\n",
      "Finished page 4\n",
      "Finished page 5\n",
      "Finished page 6\n",
      "Finished page 7\n",
      "Finished page 8\n",
      "Finished page 9\n",
      "Finished page 10\n",
      "Finished page 11\n",
      "Finished page 12\n",
      "Finished page 13\n",
      "Finished page 14\n",
      "Finished page 15\n",
      "Finished page 16\n",
      "Finished page 17\n",
      "Finished page 18\n",
      "Finished page 19\n",
      "Finished page 20\n",
      "Finished page 21\n",
      "Finished page 22\n",
      "Finished page 23\n",
      "Finished page 24\n",
      "Finished page 25\n",
      "Finished page 26\n",
      "Finished page 27\n",
      "Finished page 28\n",
      "Finished page 29\n",
      "Finished page 30\n",
      "Finished page 31\n",
      "Finished page 32\n",
      "Finished page 33\n",
      "Finished page 34\n",
      "Finished page 35\n",
      "Finished page 36\n",
      "Finished page 37\n",
      "Finished page 38\n",
      "Finished page 39\n",
      "Finished page 40\n",
      "SCRIPT RUNTIME : 0:07:20.359000\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "startTime = datetime.now()\n",
    "counter = 1\n",
    "\n",
    "for link in links:\n",
    "    temp = scrape_panel(set_url(link))\n",
    "    data = pd.concat([data,temp], ignore_index=True)\n",
    "    print \"Finished page\",counter\n",
    "    counter+=1\n",
    "    \n",
    "print \"SCRIPT RUNTIME :\",datetime.now() - startTime\n",
    "file_name = '../project-6-apis-randomforests/data.csv'\n",
    "data.to_csv(file_name, index = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tt0110912']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[t]*[t]\\d+',str(data['id'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['id'] = data['id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a function to parse out all the extra characters pulled from the id **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id(string):\n",
    "    return re.findall(r'[t]*[t]\\d+', string)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['id'] = data['id'].apply(get_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0110912</td>\n",
       "      <td>Pulp Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt1872181</td>\n",
       "      <td>The Amazing Spider-Man 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0111161</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0076759</td>\n",
       "      <td>Star Wars: Episode IV - A New Hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0088763</td>\n",
       "      <td>Back to the Future</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                name\n",
       "0  tt0110912                        Pulp Fiction\n",
       "1  tt1872181            The Amazing Spider-Man 2\n",
       "2  tt0111161            The Shawshank Redemption\n",
       "3  tt0076759  Star Wars: Episode IV - A New Hope\n",
       "4  tt0088763                  Back to the Future"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
